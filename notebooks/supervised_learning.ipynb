{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "This is being used to try out and visualize different methods of classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier as RF\n",
    "from sklearn.ensemble import GradientBoostingClassifier as GB\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.neural_network import MLPClassifier as MLP\n",
    "from sklearn.neighbors import KNeighborsClassifier as KN\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier as GPC\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier as DT\n",
    "from sklearn.ensemble import AdaBoostClassifier as ABC\n",
    "from sklearn.naive_bayes import GaussianNB as GNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pkl_file = open('../data/df.pkl', 'rb')\n",
    "df = pickle.load(pkl_file)\n",
    "pkl_file.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = ['top_elev_(ft)', \n",
    "            'bottom_elev_(ft)', \n",
    "            'vert_rise_(ft)', \n",
    "            'slope_length_(ft)', \n",
    "            'avg_width_(ft)', \n",
    "            'slope_area_(acres)', \n",
    "            'avg_grade_(%)', \n",
    "            'max_grade_(%)', \n",
    "            'groomed']\n",
    "\n",
    "X = df[features].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = df['colors'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "ss.fit(X_train)\n",
    "X_train = ss.transform(X_train)\n",
    "X_test = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf = RF()\n",
    "lr = LR()\n",
    "gb = GB()\n",
    "mlp = MLP()\n",
    "kn = KN()\n",
    "svc = SVC()\n",
    "gpc = GPC()\n",
    "rbf = RBF()\n",
    "dt = DT()\n",
    "abc = ABC()\n",
    "gnb = GNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_model(name, model,X_train,y_train,X_test,y_test):\n",
    "    model.fit(X_train,y_train)\n",
    "    print(\"{} train score: {}\".format(name, model.score(X_train,y_train)))\n",
    "    print(\"{} test score: {}\".format(name, model.score(X_test,y_test)))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names = ['Random Forest',\n",
    "         'Logistic Regression',\n",
    "         'Gradient Boosting Classifier',\n",
    "         'MLP Classifier',\n",
    "         'KNeighbors Classifier',\n",
    "         'SVC',\n",
    "         'Gaussian Process Classifier',\n",
    "         'Decision Tree Classifier',\n",
    "         'AdaBoost Classifier',\n",
    "         'Gaussian Naive Bayes']\n",
    "models = [rf,lr,gb,mlp,kn,svc,gpc,dt,abc,gnb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest train score: 0.9814077025232404\n",
      "Random Forest test score: 0.728494623655914\n",
      "\n",
      "\n",
      "Logistic Regression train score: 0.6812749003984063\n",
      "Logistic Regression test score: 0.6801075268817204\n",
      "\n",
      "\n",
      "Gradient Boosting Classifier train score: 0.9654714475431607\n",
      "Gradient Boosting Classifier test score: 0.7123655913978495\n",
      "\n",
      "\n",
      "MLP Classifier train score: 0.8100929614873837\n",
      "MLP Classifier test score: 0.7231182795698925\n",
      "\n",
      "\n",
      "KNeighbors Classifier train score: 0.8193891102257637\n",
      "KNeighbors Classifier test score: 0.7043010752688172\n",
      "\n",
      "\n",
      "SVC train score: 0.7835325365205843\n",
      "SVC test score: 0.717741935483871\n",
      "\n",
      "\n",
      "Gaussian Process Classifier train score: 0.8300132802124834\n",
      "Gaussian Process Classifier test score: 0.696236559139785\n",
      "\n",
      "\n",
      "Decision Tree Classifier train score: 1.0\n",
      "Decision Tree Classifier test score: 0.6666666666666666\n",
      "\n",
      "\n",
      "AdaBoost Classifier train score: 0.6042496679946879\n",
      "AdaBoost Classifier test score: 0.5456989247311828\n",
      "\n",
      "\n",
      "Gaussian Naive Bayes train score: 0.7184594953519257\n",
      "Gaussian Naive Bayes test score: 0.7204301075268817\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, model in zip(names,models):\n",
    "    check_model(name, model,X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying w/ resort AND color as target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_resortcolor = zip(df.resort.values,df.colors.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y2 = [' '.join(x) for x in list(y_resortcolor)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(df[features].values, y2, test_size=0.33, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ss2 = StandardScaler()\n",
    "ss2.fit(X_train2)\n",
    "X_train2 = ss2.transform(X_train2)\n",
    "X_test2 = ss2.transform(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest train score: 0.9893758300132802\n",
      "Random Forest test score: 0.3467741935483871\n",
      "\n",
      "\n",
      "Logistic Regression train score: 0.3545816733067729\n",
      "Logistic Regression test score: 0.2903225806451613\n",
      "\n",
      "\n",
      "Gradient Boosting Classifier train score: 1.0\n",
      "Gradient Boosting Classifier test score: 0.3387096774193548\n",
      "\n",
      "\n",
      "MLP Classifier train score: 0.5245683930942895\n",
      "MLP Classifier test score: 0.3575268817204301\n",
      "\n",
      "\n",
      "KNeighbors Classifier train score: 0.5391766268260292\n",
      "KNeighbors Classifier test score: 0.3118279569892473\n",
      "\n",
      "\n",
      "SVC train score: 0.4395750332005312\n",
      "SVC test score: 0.30913978494623656\n",
      "\n",
      "\n",
      "Gaussian Process Classifier train score: 0.6321381142098274\n",
      "Gaussian Process Classifier test score: 0.3521505376344086\n",
      "\n",
      "\n",
      "Decision Tree Classifier train score: 1.0\n",
      "Decision Tree Classifier test score: 0.2903225806451613\n",
      "\n",
      "\n",
      "AdaBoost Classifier train score: 0.18725099601593626\n",
      "AdaBoost Classifier test score: 0.16129032258064516\n",
      "\n",
      "\n",
      "Gaussian Naive Bayes train score: 0.31208499335989376\n",
      "Gaussian Naive Bayes test score: 0.2661290322580645\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, model in zip(names,models):\n",
    "    check_model(name, model,X_train2,y_train2,X_test2,y_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying to correct for overfitting in RF and GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def param_search(names, models, params, X_train, X_test, y_train, y_test):\n",
    "    best_models = []\n",
    "    for name, model, param in zip(names, models, params):\n",
    "        print(\"\\n########## {} model ##########\".format(name))\n",
    "        clf = GridSearchCV(model, param, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "        clf.fit(X_train, y_train)\n",
    "        print(\"Best parameters set found on development set:\")\n",
    "        print(clf.best_params_)\n",
    "        print('Best score: {:.4f}'.format(clf.best_score_))\n",
    "        best_models.append(clf.best_estimator_)\n",
    "    return best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names2 = ['Random Forest', 'Gradient Boosting Classifier']\n",
    "models2 = [RF(),GB()]\n",
    "RFparams = [{'n_estimators': [10, 20, 30], 'criterion': ['gini', 'entropy'], 'max_depth': [1, 3, 10], 'min_samples_split': [5, 10, 20], 'min_samples_leaf': [5, 10, 20], 'max_features': ['sqrt', 'log2'], 'max_leaf_nodes': [50, 100, 500], 'bootstrap': [True, False], 'warm_start': [False, True], 'class_weight': [None, 'balanced']}]\n",
    "GBCparams = [{'learning_rate': np.logspace(-2, 0, num=3), 'max_depth': [1, 3, 10], 'min_samples_leaf': [1, 3, 10], 'subsample': [1.0, 0.5], 'max_features': [None, 'sqrt'], 'n_estimators': [100]}]\n",
    "params2 = [RFparams,GBCparams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "########## Random Forest model ##########\n",
      "Best parameters set found on development set:\n",
      "{'bootstrap': True, 'class_weight': 'balanced', 'criterion': 'gini', 'max_depth': 10, 'max_features': 'log2', 'max_leaf_nodes': 100, 'min_samples_leaf': 5, 'min_samples_split': 10, 'n_estimators': 30, 'warm_start': True}\n",
      "Best score: 0.7570\n",
      "\n",
      "########## Gradient Boosting Classifier model ##########\n",
      "Best parameters set found on development set:\n",
      "{'learning_rate': 0.01, 'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 10, 'n_estimators': 100, 'subsample': 0.5}\n",
      "Best score: 0.7517\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "             criterion='gini', max_depth=10, max_features='log2',\n",
       "             max_leaf_nodes=100, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=5,\n",
       "             min_samples_split=10, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=30, n_jobs=1, oob_score=False, random_state=None,\n",
       "             verbose=0, warm_start=True),\n",
       " GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "               learning_rate=0.01, loss='deviance', max_depth=10,\n",
       "               max_features='sqrt', max_leaf_nodes=None,\n",
       "               min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "               min_samples_leaf=10, min_samples_split=2,\n",
       "               min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "               presort='auto', random_state=None, subsample=0.5, verbose=0,\n",
       "               warm_start=False)]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_search(names2,models2,params2,X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_RF = RF(bootstrap=True, class_weight='balanced',\n",
    "             criterion='gini', max_depth=10, max_features='log2',\n",
    "             max_leaf_nodes=100, min_impurity_decrease=0.0,\n",
    "             min_impurity_split=None, min_samples_leaf=5,\n",
    "             min_samples_split=10, min_weight_fraction_leaf=0.0,\n",
    "             n_estimators=30, n_jobs=1, oob_score=False, random_state=None,\n",
    "             verbose=0, warm_start=True)\n",
    "best_GB =  GB(criterion='friedman_mse', init=None,\n",
    "               learning_rate=0.01, loss='deviance', max_depth=10,\n",
    "               max_features='sqrt', max_leaf_nodes=None,\n",
    "               min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "               min_samples_leaf=10, min_samples_split=2,\n",
    "               min_weight_fraction_leaf=0.0, n_estimators=100,\n",
    "               presort='auto', random_state=None, subsample=0.5, verbose=0,\n",
    "               warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best RF train score: 0.8778220451527224\n",
      "Best RF test score: 0.7123655913978495\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "check_model('Best RF', best_RF,X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best GB train score: 0.8326693227091634\n",
      "Best GB test score: 0.7123655913978495\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "check_model('Best GB', best_GB,X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clearly it is hard to classify runs by their color - since different resorts have different criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
